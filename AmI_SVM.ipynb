{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fMZsu2E0z6zs",
        "pgOZlx9UhJgw"
      ],
      "mount_file_id": "1ZEsWI_JwEAV5qK3-sU-L_qUitV1r3kCY",
      "authorship_tag": "ABX9TyNVMLRufmMvggdXBOOcVZ94",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plopezmp/AmI/blob/main/AmI_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXce444sFl5v"
      },
      "source": [
        "Las figuras de las secciones *Objetivo de entrenamiento*, *Gaussian RBF kernel*, y *Regresión con SVM* están obtenidas del libro:\n",
        "\n",
        "* A. Géron. *Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems*. O’Reilly Media, 2019."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE4r7Isy8TVg"
      },
      "source": [
        "**Preambulo:**\n",
        "\n",
        "Montamos nuestro `Google Drive` donde tenemos guardado el notebook de Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrcb3RaF8WZV"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jshrG51p8aYl"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/')\n",
        "%pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfYCcndjrpFs"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPK31eO57vDe"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats\n",
        "\n",
        "# seaborn plotting por defecto\n",
        "import seaborn as sns; sns.set()\n",
        "from ipywidgets import interact"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJYx-bVN8E5n"
      },
      "source": [
        "<a id='sec_SVM'></a>\n",
        "# Clasificación con Máquinas de Vector Soporte (SVM) lineal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DBVdbp0s9dL"
      },
      "source": [
        "*Support Vector Machine* (SVM) es un potente algoritmo de aprendizaje supervisado que se utiliza para **clasificación** o **regresión**.\n",
        "\n",
        "SVM es un clasificador **discriminativo**: es decir, trazan una frontera entre grupos de datos.\n",
        "\n",
        "SVM está indicado para clasificación datasets pequeños o medios con gran cantidad de features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-EyqSRMbCl6"
      },
      "source": [
        "La idea fundamental tras SVMs se puede explicar con algunas figuras. Vamos a crear dos *blobs* de datos que podamos separar fácilmente con una línea recta (*linealmente separables*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9gk_pMPpD9c"
      },
      "source": [
        "from sklearn.datasets import make_blobs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKnZ4vbxvMim"
      },
      "source": [
        "make_blobs?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HE0ql9QvOid"
      },
      "source": [
        "X, y = make_blobs(n_samples=50,\n",
        "                  n_features=2,\n",
        "                  centers=2,\n",
        "                  random_state=0,\n",
        "                  cluster_std=0.60\n",
        "                  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPSZaK0wxi1_"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "U2vFvQwcFcHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLE3H7rRwgRv"
      },
      "source": [
        "plt.scatter(X[:,0],X[:,1], c=y, s=50, cmap='coolwarm')\n",
        "plt.xlabel('$x_1$', fontsize=15)\n",
        "plt.ylabel('$x_2$', fontsize=15);\n",
        "#plt.savefig('decbaund.pdf', format='pdf', dpi=500, bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk48Qar0ybwP"
      },
      "source": [
        "Un clasificador discriminativo va a trazar una frontera entre los dos grupos. Sin embargo, ¿cómo debe ser esa frontera? Podemos trazar varias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_imYzGp5xRKj"
      },
      "source": [
        "xfit = np.linspace(-1,3.5)\n",
        "plt.scatter(X[:,0],X[:,1], c=y, s=50, cmap='coolwarm')\n",
        "\n",
        "for w, b in [(1,0.65), (0.5,1.6), (-0.2,2.9)]:\n",
        "  plt.plot(xfit, w*xfit+b, '-k',)\n",
        "\n",
        "#plt.xlim(-1,3.5);\n",
        "plt.xlabel('$x_1$', fontsize=15)\n",
        "plt.ylabel('$x_2$', fontsize=15);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leIqQk_QzCs6"
      },
      "source": [
        "¿Cuál es la más adecuada?\n",
        "Las tres líneas discriminan perfectamente los dos grupos de datos. Sin embargo, en función de cuál seleccionemos, un nuevo dato se podrá clasificar de una forma diferente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRxSHW4a0-W1"
      },
      "source": [
        "<a id='subsec_Margin'></a>\n",
        "## Classification margin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY9pMqrp2Tat"
      },
      "source": [
        "Las máquinas de vector soporte (SVM) son una forma de abordar esto.\n",
        "\n",
        "Lo que hace el método es no solo dibujar una línea, sino considerar una *región* en torno a la línea de un ancho determinado.\n",
        "\n",
        "A continuación, se muestra un ejemplo de cómo podría verse esa región:\n",
        "\n",
        "Debemos seleccionar una frontera que *maximice* el margen, la separación entre grupos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l5d1vz0zGAl"
      },
      "source": [
        "xfit = np.linspace(-1,3.5) # 50 muestras por defecto\n",
        "plt.scatter(X[:,0],X[:,1], c=y, s=50, cmap='coolwarm')\n",
        "\n",
        "for m, b, d in [(1,0.65, 0.33), (0.5,1.6, 0.55), (-0.2,2.9, 0.2)]:\n",
        "  yfit = m*xfit+b\n",
        "  plt.plot(xfit, yfit, '-k')\n",
        "  plt.fill_between(xfit, yfit-d, yfit+d, edgecolor='none', color='#AAAAAA', alpha=0.4)\n",
        "\n",
        "\n",
        "plt.xlim(-1,3.5);\n",
        "# plt.savefig('decbaund1.pdf', format='pdf', dpi=500, bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifexWY6E4G1s"
      },
      "source": [
        "La frontera que mayor margen tiene es la que separa mejor los dos grupos.\n",
        "\n",
        "Esta es la idea detrás de las máquinas de vector soporte.\n",
        "Podemos pensar un clasificador SVM como el ajuste de de la *calle* más ancha posible (representada por las franjas semitrasparentes y paralelas en torno a la línea que separa las clases.\n",
        "\n",
        "Este es un problema que casi podríamos resolver a mano. Sin embargo, ¿cómo lo haríamos si el problema tiene más features de entrada?\n",
        "\n",
        "El resultado exacto se obtiene resolviendo el problema de optimización que podríamos plantear.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC7rLTMNj5mZ"
      },
      "source": [
        "# Decision function and predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UDwIDha9E62"
      },
      "source": [
        "En logistic regression usamos el vector $\\boldsymbol{\\theta}$, que incluía el parámetro bias $\\theta_0$. Además, añadíamos la entrada $x_0=1$ a todas las instancias del dataset.\n",
        " En SVM vamos a usar una nomenclatura más conveniente que se ajuste al código que tenemos implementado:\n",
        "\n",
        "- Vector de features: ${\\boldsymbol{\\theta}}$\n",
        "- término de bias: $b$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoCTHyOkAaWQ"
      },
      "source": [
        "El clasificador SVM lineal predice la clase de una nueva instancia ${\\bf x}$ computando la *función de decisión*:\n",
        "\n",
        "$$\n",
        "  {\\bf w}^\\intercal {\\bf x} +b = w_1 x_1 + \\cdots + w_n x_n\n",
        "$$\n",
        "\n",
        "si el resultado es positivo, la clase estimada es la clase *positiva* (1), y, en caso contrario es la clase *negativa* (0).\n",
        "\n",
        "$$\n",
        "\\hat{y} =\n",
        "\\begin{cases}\n",
        "  0 &\\mbox{si } {\\bf w}^\\intercal {\\bf x} +b < 0 \\\\\n",
        "  1 & \\mbox{si } {\\bf w}^\\intercal {\\bf x} +b \\geq 0\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV5V-4H5QeQh"
      },
      "source": [
        "La siguiente figura muetra el plano que representa la función de decisión si resolvemos nuestro problema de los dos blobs con el modelo SVM lineal. Podemos usar `LinearSVC()` o bien usar una instancia de `SVC`, *Support Vector Classifier*, con el `kernel='linear'`. Si el dataset es muy grande usaremos `LinearSVC()` que resulta más rápido. En nuestro caso, usamos dataset pequeños usamos esta segunda versión por sencillez:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHgFk07yJCbY"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.svm import SVC # \"Support Vector Calssifier\"\n",
        "\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X, y)\n",
        "\n",
        "def plot_3D_decision_function(ax, w, b,  x1_lim=[-1, 3.5], x2_lim=[-1, 6]):\n",
        "    x1s = np.linspace(x1_lim[0], x1_lim[1], 20)\n",
        "    x2s = np.linspace(x2_lim[0], x2_lim[1], 20)\n",
        "    x1, x2 = np.meshgrid(x1s, x2s)\n",
        "    xs = np.c_[x1.ravel(), x2.ravel()]\n",
        "    df = (xs.dot(w) + b).reshape(x1.shape)\n",
        "    m = 1 / np.linalg.norm(w)\n",
        "    boundary_x2s = -x1s*(w[0]/w[1])-b/w[1]\n",
        "    margin_x2s_1 = -x1s*(w[0]/w[1])-(b-1)/w[1]\n",
        "    margin_x2s_2 = -x1s*(w[0]/w[1])-(b+1)/w[1]\n",
        "    ax.plot_surface(x1s, x2, np.zeros_like(x1),\n",
        "                    color=\"b\", alpha=0.1, cstride=100, rstride=100)\n",
        "    ax.plot(x1s, boundary_x2s, 0, \"k-\", linewidth=2, label=r\"$h=0$\")\n",
        "    ax.plot(x1s, margin_x2s_1, 0, \"k--\", linewidth=2, label=r\"$h=\\pm 1$\")\n",
        "    ax.plot(x1s, margin_x2s_2, 0, \"k--\", linewidth=2)\n",
        "    # función de decisión\n",
        "    ax.plot_wireframe(x1, x2, df, alpha=0.2, color=\"k\")\n",
        "    plt.scatter(X[:,0],X[:,1], c=y, s=50, cmap='coolwarm')\n",
        "    #ax.axis(x1_lim + x2_lim)\n",
        "    ax.text(2, -0.5, 5.25, \"Decision function $h$\", fontsize=15)\n",
        "    ax.set_xlabel(r\"$x_1$\", fontsize=15)\n",
        "    ax.set_ylabel(r\"$x_2$\", fontsize=15)\n",
        "    ax.set_zlabel(r\"$h = \\mathbf{w}^T \\mathbf{x} + b$\", fontsize=14)\n",
        "    ax.legend(loc=\"upper left\", fontsize=16)\n",
        "\n",
        "fig = plt.figure(figsize=(11, 6))\n",
        "ax1 = fig.add_subplot(111, projection='3d')\n",
        "ax1.view_init(elev=30, azim=30)\n",
        "plot_3D_decision_function(ax1, w=clf.coef_[0], b=clf.intercept_[0]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yx8lZ_VTgc8"
      },
      "source": [
        "La frontera de decisión es la línea se separación (linea continua en la figura) donde la función de decisión es igual a $0$; corresponde a la instersección entre los dos planos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd48EtpY5E5V"
      },
      "source": [
        "Más sencillo resulta visualizar este resultado en 2D:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqEQegKxvLT3"
      },
      "source": [
        "Vamos a crear una función para dibujar la frontera de decisión. La función del modelo a dibujar se llama `clf.decision_function`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aRsd2OWzJPa"
      },
      "source": [
        "help(plt.contour)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_svc_decision_function(clf, ax=None):\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    # Create grid to evaluate model\n",
        "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 30),\n",
        "                         np.linspace(ylim[0], ylim[1], 30))\n",
        "\n",
        "    xy = np.vstack([xx.ravel(), yy.ravel()]).T\n",
        "\n",
        "    P = clf.decision_function(xy).reshape(xx.shape)\n",
        "\n",
        "    # Plot decision boundary and margins\n",
        "    ax.contour(xx, yy, P, colors='k',\n",
        "               levels=[-1, 0, 1], alpha=0.5,\n",
        "               linestyles=['--', '-', '--'])\n",
        "\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)\n"
      ],
      "metadata": {
        "id": "r4-ieo86E3z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X[:,0],X[:,1], c=y, s=50, cmap='coolwarm')\n",
        "plot_svc_decision_function(clf)\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "            s=200,\n",
        "            facecolors='none',\n",
        "            edgecolor='black'\n",
        "            )\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LaraAmaaFniV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWNVFPsS0G_r"
      },
      "source": [
        "Observe que las líneas punteadas tocan un par de puntos: estos puntos son las piezas fundamentales de este ajuste, y se conocen como ***vectores de soporte*** (dando al algoritmo su nombre).\n",
        "En `scikit-learn`, estos se almacenan en el atributo `support_vectors_` del clasificador:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxPESw6J63qg"
      },
      "source": [
        "Usemos la funcionalidad `interact` de `IPython` para explorar cómo la distribución de puntos afecta los vectores de soporte y el ajuste discriminativo."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SVC?"
      ],
      "metadata": {
        "id": "WZ9b9_xWIzi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import interact\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "def plot_svm(N=10):\n",
        "    plt.clf()  # Clear current plot\n",
        "    X, y = make_blobs(n_samples=250, centers=2,\n",
        "                      random_state=0, cluster_std=0.70)\n",
        "    X = X[:N]\n",
        "    y = y[:N]\n",
        "    # C=1 dafault value\n",
        "    clf = SVC(C=1,kernel='linear')\n",
        "    clf.fit(X, y)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='coolwarm')\n",
        "    plt.xlim(-1, 4)\n",
        "    plt.ylim(-1, 6)\n",
        "    plot_svc_decision_function(clf, plt.gca())\n",
        "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "                s=200, facecolors='none')\n",
        "    plt.show()  # Show the updated plot\n",
        "\n",
        "interact(plot_svm, N=[10,30,100, 120, 160, 200, 250]);\n"
      ],
      "metadata": {
        "id": "Lqsxz7swISBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_RoPoEB9E7n"
      },
      "source": [
        "Las líneas punteadas representan donde la función de decisión es igual a $1$ o $-1$. Son líneas paralelas en torno a la frontera de decisión ($h=0$), y forman un margen alrededor.\n",
        "\n",
        "Entrenar un clasificador lineal SVM significa encontrar los valores de ${\\bf w}$ y de $b$ que hacen los márgenes tan anchos como sea posible, evitando instancias entre esos márgenes, (*hard margin*), o, limitándolos, (*soft margin*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdrbBOffxS1S"
      },
      "source": [
        "## Objetivo de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk1qheuXxc4M"
      },
      "source": [
        "En el ejemplo que venimos estudiando con dos features hemos visualizado la función de decisión. La pendiente de la función de decisión es igual a la norma del vector de pesos $\\|{\\bf w}\\|$.\n",
        "\n",
        "Si dividimos la pendiente por $2$, los puntos donde la función de decisión vale $\\pm 1$ estarán el doble de lejos de la frontera de descisión. Es decir, dividiendo la pendiente por 2, duplicamos el margen.\n",
        "Esto lo podemos visualizar fácilmente en 2D, usando una feature $x_1$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuD96fHQzEUC"
      },
      "source": [
        "def plot_2D_decision_function(w, b, ylabel=True, x1_lim=[-3, 3]):\n",
        "    x1 = np.linspace(x1_lim[0], x1_lim[1], 200)\n",
        "    y = w * x1 + b\n",
        "    m = 1 / w\n",
        "\n",
        "    plt.plot(x1, y) # función de decisión h = w*x1 + b\n",
        "    plt.plot(x1_lim, [1, 1], \"k:\") # margen h=1\n",
        "    plt.plot(x1_lim, [-1, -1], \"k:\") # margen h=-1\n",
        "    plt.axhline(y=0, color='k') # eje horizontal\n",
        "    plt.axvline(x=0, color='k') # eje vertical\n",
        "    plt.plot([m, m], [0, 1], \"k--\") # si w=1, va de (m,0) -- (m,1)\n",
        "    plt.plot([-m, -m], [0, -1], \"k--\")\n",
        "    plt.plot([-m, m], [0, 0], \"k-o\", linewidth=3) # 'margen'\n",
        "    plt.axis(x1_lim + [-2, 2])\n",
        "    plt.xlabel(r\"$x_1$\", fontsize=16)\n",
        "    if ylabel:\n",
        "        plt.ylabel(r\"$w_1 x_1$  \", rotation=0, fontsize=16)\n",
        "    plt.title(r\"$w_1 = {}$\".format(w), fontsize=16)\n",
        "\n",
        "plt.figure(figsize=(12, 3.2))\n",
        "plt.subplot(121)\n",
        "plot_2D_decision_function(1, 0)\n",
        "plt.subplot(122)\n",
        "plot_2D_decision_function(0.5, 0, ylabel=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh2UXAeI3lvD"
      },
      "source": [
        "Cuanto más pequeña se la pendiente, es decir el vector ${\\bf w}$ más grande será el margen de separación de las clases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhFKrV-r37PT"
      },
      "source": [
        "Por tanto, querremos *minimizar* $\\|{\\bf w}\\|$ para tener un margen grande.\n",
        "\n",
        "Si también queremos evitar que haya instancias dentro del margen (*hard margin*), entonces necesitamos una función de decisión que haga $>1$ todas las instancias positivas, y, que haga $<-1$ para todas las negativas.\n",
        "\n",
        "Si definimos $t^{(i)}$ de la siguiente forma:\n",
        "\n",
        "$$\n",
        "t^{(i)} =\n",
        "\\begin{cases}\n",
        "  -1 &\\mbox{si }\\, y^{(i)} = 0 \\\\\n",
        "  1 & \\mbox{si }\\, y^{(i)} = 1\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "entonces podemos expresar la restricción de la forma:$\\hspace{3mm}$ $t^{(i)}\\left({\\bf w}^\\intercal {\\bf x}^{(i)} + b \\right)\\geq 1$ para todas las instancias\n",
        "\n",
        "Por tanto, podemos expresar el objetivo del clasificador lineal (*hard margin* version) como el siguiente problema de optimización:\n",
        "\n",
        "\\begin{aligned}\n",
        "& \\underset{{\\bf w},b}{\\text{minimize}}\n",
        "& & \\frac{1}{2}{\\bf w}^\\intercal{\\bf w} & \\\\\n",
        "& \\text{subject to}\n",
        "& & t^{(i)}\\left({\\bf w}^\\intercal {\\bf x}^{(i)} + b \\right)\\geq 1 & \\forall i=1,2,\\cdots,m\n",
        "\\end{aligned}\n",
        "\n",
        "\n",
        "\n",
        "No minimizamos directamente $\\|{\\bf w}\\|$ que es una función no continua en ${\\bf w}=0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKwUkl9kuHVk"
      },
      "source": [
        "## Soft margin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akLiME9PfNQp"
      },
      "source": [
        "Hemos impuesto que las instancias estén fuera de la *calle*. Existen dos problemas con clasificar *hard*:\n",
        "\n",
        "1. Sólo funciona bien si los datos son linealmente separables\n",
        "2. Resulta sensible a *outliers* y no *generaliza* bien.\n",
        "\n",
        "Los márgenes cambian si introducimos nuevas instancias. La consecuencia es similar a *overfitting* que hemos visto en los problemas de regresión.\n",
        "\n",
        "Para evitarlo, se busca flexibilizar el modelo. El objetivo consiste en encontrar un balance entre tener una *calle* lo más ancha posible y lilmitar las instancias que caen dentro de los márgenes (*margin violations*). Las instancias pordrían situarse dentro de la *calle* e incluso *en el lado contrario*, donde están las instancias clasificadas diferente.\n",
        "\n",
        "Para conseguir una clasificación con margen *soft* debemos cambiar el objetivo introduciendo una variable que relaje la condición impuesta de no permitir instancias dentro, $\\zeta^{(i)}\\geq 0$ para cada instancia: $\\zeta^{(i)}$ mide cuanto se permite que la instancia $i$ *viole* el margen.\n",
        "\n",
        "Ahora tenemos dos objetivos contrapuestos:\n",
        "\n",
        "* hacer $\\zeta^{(i)}$ lo más pequeña posible para no permitir violaciones de márgen, y\n",
        "* hacer $\\frac{1}{2}{\\bf w}^\\intercal{\\bf w}\\,$ lo más pequeño posible para para *agrandar* el márgen.\n",
        "\n",
        "Para modular estos $2$ objetivos introducimos el hiperparámetro $C$ y el problema de optimización del clasificador es:\n",
        "\n",
        "\\begin{aligned}\n",
        "& \\underset{{\\bf w},b,\\zeta}{\\text{minimize}}\n",
        "& & \\frac{1}{2}{\\bf w}^\\intercal{\\bf w}\\, + C \\sum_{i=1}^m \\zeta^{(i)} &  \\\\\n",
        "& \\text{subject to}\n",
        "& & t^{(i)}\\left({\\bf w}^\\intercal {\\bf x}^{(i)} + b \\right)\\geq 1-\\zeta^{(i)} & \\zeta^{(i)}\\geq 0,\\, \\forall i=1,2,\\cdots,m\n",
        "\\end{aligned}\n",
        "\n",
        "que se trata de un problema de optimización convexo con restricción lineal. La resolución de este problema queda fuera de nuestro objetivo; usaremos `sklearn` que implementa varios solvers para estos problemas.\n",
        "\n",
        "El hiperparámetro $C$ es uno de los que podemos ajustar. Si establecemos un valor bajo ($\\approx 0$) tenemos un modelo que permitirá violaciones de margen. Con valores altos tendremos un modelo *hard* que no permitirá que haya instancias en el margen de decisión. Vamos a mira el valor por defecto y lo subiremos para tener un clasificador *hard*.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmFVxIqqRXkK"
      },
      "source": [
        "SVC?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYnSxNPHRfw0"
      },
      "source": [
        "Por defecto $C=1$; vamos a subirlo a $100$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import interact\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "def plot_svm(N=10):\n",
        "    fig, ax = plt.subplots()  # Create a new figure and axis\n",
        "    X, y = make_blobs(n_samples=250, centers=2,\n",
        "                      random_state=0, cluster_std=0.70)\n",
        "    X = X[:N]\n",
        "    y = y[:N]\n",
        "    clf = SVC(kernel='linear', C=100)\n",
        "    clf.fit(X, y)\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='coolwarm')\n",
        "    ax.set_xlim(-1, 4)\n",
        "    ax.set_ylim(-1, 6)\n",
        "    plot_svc_decision_function(clf, ax)\n",
        "    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "                s=200, facecolors='none')\n",
        "    plt.show()  # Show the updated plot\n",
        "\n",
        "interact(plot_svm, N=[10,30,100, 120, 160, 200, 250]);\n"
      ],
      "metadata": {
        "id": "jjIX3xqqKFH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDN9pRNhRwK9"
      },
      "source": [
        "Vemos como reduce el márgen para que no haya violaciones de márgen conforme subimos la cantidad de instancias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5yFSZEVTxUK"
      },
      "source": [
        "# Clasificación SVM no-lineal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx2UW0afVLKR"
      },
      "source": [
        "El datasetde los ejemplos que acabamos de ver tiene instancias que son linealmente separables. Sin embargo, podemos tener datasets con instancias que no lo son."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9P_viE--Abx"
      },
      "source": [
        "Veamos el siguiente ejemplo de clasificación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYB9aahk7Lgc"
      },
      "source": [
        "from sklearn.datasets import make_circles\n",
        "X, y = make_circles(100, factor=.1, noise=.1, random_state=0)\n",
        "\n",
        "clf = SVC(kernel='linear').fit(X, y)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='coolwarm')\n",
        "plot_svc_decision_function(clf);\n",
        "plt.xlabel(r'$x_1$', fontsize=14)\n",
        "plt.ylabel(r'$x_2$', fontsize=14);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvcTBC4m-msc"
      },
      "source": [
        "Claramente, ninguna discriminación lineal separará jamás estos datos.\n",
        "\n",
        "En general, existen dos formas de abordar estos problemas:\n",
        "\n",
        "1. Añadir features polinómicas. Habitualmente usamos kernels polinómicos de mayor grado (combinación lineal de funciones base polinómicas), como ya vimos en *regresión logística*. El inconveniente es que al subir el grado del polinomio terminamos creando gran cantidad de features y no podemos usarlo con datasets complejos.\n",
        "\n",
        "2. Añadir features usando *similarity functions*, la más usada es la Gaussian *Radial Basis Function* (RBF). En este caso veremos que también podemos tener una *explosión* de features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVPHdMwI1DUq"
      },
      "source": [
        "Afortunadamente, en los dos casos se aplica la técnica del ***kernel trick***.\n",
        "El [kernel trick](https://en.wikipedia.org/wiki/Kernel_method) obtiene los mismos resultados que si incrementamos mucho la cantidad de features (incluso para polinomios de grado muy alto), sin tener que añadirlos realmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1_qf6EU0smH"
      },
      "source": [
        "## Kernel trick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy0MbLCW0rM0"
      },
      "source": [
        "La idea tras el kernel trick es, partiendo de un dataset con instancias no linealmente separables transformar esas instancias a una dimensión superior donde sí son separables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P58YVpGUnE93"
      },
      "source": [
        "Podemos visualizar la técnica del kernel trick en el dataset anterior. Viendo la disposición circular de las dos clases, a las features $x_1$ y $x_2$ añadimos una tercera $x_3$ que varíe con el radio desde el punto $(0,0)$, y pasamos de un espacio de dimensión 2D a otro 3D:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR4ks64F-NPA"
      },
      "source": [
        "x_3 = np.exp(-(X[:, 0] ** 2 + X[:, 1] ** 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33L6kPjV_Ceg"
      },
      "source": [
        "Si ahora dibujamos con nuestros datos podemos ver el efecto producido:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "def plot_3D(elev=21, azim=-13):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.scatter3D(X[:, 0], X[:, 1], x_3, c=y, s=50, cmap='coolwarm')\n",
        "    ax.view_init(elev=elev, azim=azim)\n",
        "    ax.set_xlabel(r'$x_1$', fontsize=15)\n",
        "    ax.set_ylabel(r'$x_2$', fontsize=15)\n",
        "    ax.set_zlabel(r'$x_3$', fontsize=15)\n",
        "    plt.show()\n",
        "\n",
        "interact(plot_3D, elev=(-90, 90), azim=(-180, 180));\n"
      ],
      "metadata": {
        "id": "pNGLyGHJLr3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92f_Uc1iBTJu"
      },
      "source": [
        "Podemos ver que con esta dimensión adicional, los datos se vuelven linealmente separables.\n",
        "\n",
        "Por ejemplo, podemos usar un kernel polinómico de grado 2 para realizar la clasificación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H62BaZqE_Z8K"
      },
      "source": [
        "clf = SVC(kernel='poly', degree=2, coef0=1, C=5)\n",
        "clf.fit(X, y)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='coolwarm')\n",
        "plot_svc_decision_function(clf)\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "            s=200,\n",
        "            facecolors='none',\n",
        "            edgecolor='black'\n",
        "            );\n",
        "\n",
        "plt.xlabel(r'$x_1$', fontsize=14)\n",
        "plt.ylabel(r'$x_2$', fontsize=14);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vshkmRKyxKCe"
      },
      "source": [
        "El hiperparámetro `coef0` (sólo para `kernel='poly'`) controla cuánto está influenciado el modelo por polinomios de grado alto. Podemos probar con un grado 10, y deberíamos subir el valor de `coef0`, por ejemplo, a 20 para obtener un valor similar a este.\n",
        "\n",
        "Recordemos que podemos realizar una búsqueda de los hiperparámetros usando `GridSearchCV`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMZsu2E0z6zs"
      },
      "source": [
        "## Similarity functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWNNW_p92Ljf"
      },
      "source": [
        "Las funciones de similaridad mide cuánto se ajusta cada instancia a un punto de referencia.\n",
        "\n",
        "Por ejemplo, la siguiente figura representa un dataset de 9 puntos situados en una recta (feature $x_1$). Por lo tanto no son linealmente separables. Vamos a añadir dos puntos de referencia (*landmarks*), uno en $x_1=-2$ y otro en $x_1=1$.\n",
        "\n",
        "Las funciones de similaridad son Gaussian ***Radial Basis Functions*** (RBF):\n",
        "\n",
        "$$\n",
        "\\phi_{\\gamma}({\\bf x},l\\,)= \\exp \\left( -\\gamma \\|{\\bf x}-l\\|^2 \\right)\n",
        "$$\n",
        "con $\\gamma=0.3$.\n",
        "\n",
        "Estas funciones de similaridad tienen forma *acampanada*, tomando valor 0 en puntos distantes del landmark, y valor 1 justo en el landmark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC1baVLRybXn"
      },
      "source": [
        "def gaussian_rbf(x, landmark, gamma):\n",
        "    return np.exp(-gamma * np.linalg.norm(x - landmark, axis=1)**2)\n",
        "\n",
        "gamma = 0.3\n",
        "X1D = np.linspace(-4, 4, 9).reshape(-1, 1)\n",
        "\n",
        "x1s = np.linspace(-4.5, 4.5, 200).reshape(-1, 1)\n",
        "x2s = gaussian_rbf(x1s, -2, gamma)\n",
        "x3s = gaussian_rbf(x1s, 1, gamma)\n",
        "\n",
        "XK = np.c_[gaussian_rbf(X1D, -2, gamma), gaussian_rbf(X1D, 1, gamma)]\n",
        "yk = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n",
        "\n",
        "plt.axhline(y=0, color='k')\n",
        "plt.scatter(x=[-2, 1], y=[0, 0], s=250, alpha=0.65, c=\"yellow\")\n",
        "plt.scatter(X1D, np.zeros(9), c=yk, s=75, cmap='coolwarm')\n",
        "plt.plot(x1s, x2s, \"g--\")\n",
        "plt.plot(x1s, x3s, \"b:\")\n",
        "plt.gca().get_yaxis().set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
        "plt.xlabel(r\"$x_1$\", fontsize=17)\n",
        "plt.ylabel(r\"Similarity\", fontsize=15)\n",
        "plt.annotate(r'$\\mathbf{x}$',\n",
        "             xy=(X1D[3, 0], 0),\n",
        "             xytext=(-0.5, 0.20),\n",
        "             ha=\"center\",\n",
        "             arrowprops=dict(facecolor='black', shrink=0.1),\n",
        "             fontsize=16,\n",
        "            )\n",
        "plt.text(-2, 0.9, \"$x_2$\", ha=\"center\", fontsize=17)\n",
        "plt.text(1, 0.9, \"$x_3$\", ha=\"center\", fontsize=17)\n",
        "plt.axis([-4.5, 4.5, -0.1, 1.1]);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBNHCUdh_hXV"
      },
      "source": [
        "En la figura hemos situado 2 landmarks, uno en $x_1=-2$ y otro en $x_1=1$.\n",
        "\n",
        "Usando las funciones de similaridad con media en los dos landmarks calculamos las nuevas features.\n",
        "Por ejemplo, para la instancia $x_1=-1$ las funciones de similaridad valen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibbDKJwe9VYo"
      },
      "source": [
        "x1_example = X1D[3, 0]\n",
        "for landmark in (-2, 1):\n",
        "    k = gaussian_rbf(np.array([[x1_example]]), np.array([[landmark]]), gamma)\n",
        "    print(\"Phi({}, {}) = {}\".format(x1_example, landmark, k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsLRvGEYETMi"
      },
      "source": [
        "que corresponden a las dos nuevas features $x_2$ y $x_3$. En la siguiente figura representamos el nuevo dataset transformado (eliminado la feature original):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwOjn_FjDDr3"
      },
      "source": [
        "plt.axhline(y=0, color='k')\n",
        "plt.axvline(x=0, color='k')\n",
        "#plt.plot(XK[:, 0][yk==0], XK[:, 1][yk==0], \"bs\")\n",
        "#plt.plot(XK[:, 0][yk==1], XK[:, 1][yk==1], \"g^\")\n",
        "plt.scatter(XK[:,0], XK[:,1], c=yk, s=75, cmap='coolwarm')\n",
        "plt.xlabel(r\"$x_2$\", fontsize=15)\n",
        "plt.ylabel(r\"$x_3$  \", fontsize=15, rotation=0)\n",
        "plt.annotate(r'$\\phi\\left(\\mathbf{x}\\right)$',\n",
        "             xy=(XK[3, 0], XK[3, 1]),\n",
        "             xytext=(0.65, 0.50),\n",
        "             ha=\"center\",\n",
        "             arrowprops=dict(facecolor='black', shrink=0.1),\n",
        "             fontsize=15,\n",
        "            )\n",
        "# creamos una frontera lineal arbitraria:\n",
        "plt.plot([-0.1, 1.1], [0.57, -0.1], \"g--\", linewidth=3)\n",
        "plt.axis([-0.1, 1.1, -0.1, 1.1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmsijl75IPv5"
      },
      "source": [
        "Resulta difícil decidir dónde debemos situar los landmarks. La forma de proceder en situar un landmark en todas las instancias del dataset.\n",
        "\n",
        "De esta forma creamos bastantes dimensiones y aumentamos la posibilidad de que el dates transformado sea linealmente separable. Si eliminamos las features originales, después de la transformación nos quedará un dataset con $m$ instancias y cada una con $m$ features. Si el training set es muy grande podemos terminar con una cantidad también muy grande de features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgOZlx9UhJgw"
      },
      "source": [
        "## Gaussian RBF kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0icbfndxhYWW"
      },
      "source": [
        "Tanto el método de añadir features polinómicas como el método de funciones/features de similaridad pueden estenderse y aplicarse a otros algoritmos de aprendizaje máquina. Sin embargo, pueden resultar computacionalmente costosos.\n",
        "\n",
        "El kernel trick va a posibilitar obtener resultados comoparables a si hubiéramos añadido muchas features de similaridad. Vamos a probar la clase `SVC` con Gaussian RBF kernel.\n",
        "\n",
        "Vamos a clasificar el `moons` dataset: es un dataset sencillo de clasificación binaria, donde los datos forman dos semicírculos entrelazados. Lo generamos usando `make_moons()` (similar a como hemos hecho con `make_blobs` y `make_circles`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkcdyxrIqjWe"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "X, y = make_moons(\n",
        "    n_samples=100,\n",
        "    noise=0.15,\n",
        "    random_state=42\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ8dGUEuqSy2"
      },
      "source": [
        "def plot_dataset(X, y, axes):\n",
        "    #plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
        "    #plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
        "    plt.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\n",
        "    plt.axis(axes)\n",
        "    plt.grid(True, which='both')\n",
        "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
        "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
        "\n",
        "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB56vTRWGSQE"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQj2C-yClmTu"
      },
      "source": [
        "clf = Pipeline([\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('svm_clf', SVC(kernel='rbf', gamma=5, C=0.001))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYmsCMRBmtId"
      },
      "source": [
        "clf.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynqusGIdxpEO"
      },
      "source": [
        "def plot_predictions(clf, axes):\n",
        "    x0s = np.linspace(axes[0], axes[1], 100)\n",
        "    x1s = np.linspace(axes[2], axes[3], 100)\n",
        "    x0, x1 = np.meshgrid(x0s, x1s)\n",
        "    X = np.c_[x0.ravel(), x1.ravel()]\n",
        "    y_pred = clf.predict(X).reshape(x0.shape)\n",
        "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
        "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
        "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-YGeSZaoNwk"
      },
      "source": [
        "gamma1, gamma2 = 0.1, 5\n",
        "C1, C2 = 0.001, 1000\n",
        "hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n",
        "\n",
        "svm_clfs = []\n",
        "for gamma, C in hyperparams:\n",
        "    clf = Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n",
        "        ])\n",
        "    clf.fit(X, y)\n",
        "    svm_clfs.append(clf)\n",
        "\n",
        "plt.figure(figsize=(11, 7))\n",
        "plt.subplots_adjust(hspace=0.4)\n",
        "for i, svm_clf in enumerate(svm_clfs):\n",
        "    plt.subplot(221 + i)\n",
        "    plot_predictions(svm_clf, [-1.5, 2.5, -1, 1.5])\n",
        "    plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
        "    gamma, C = hyperparams[i]\n",
        "    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy_RP8p95eS_"
      },
      "source": [
        "Comprobamos que incrementar $\\gamma$ hace la función de similaridad más estrecha, y, como consecuencia el efecto de cada instancia sobre los demás es menor. Cada instancia afecta a la frontera de separación en su zona lo que hace la línea más irregular. Si $\\gamma$ es pequeño las instancias tienen efecto a más distancia y la frontera se hace más suave.\n",
        "\n",
        "Por tanto, $\\gamma $ actúa como un hiperparámetro de regularización. Si tenemos *overfitting* debemos reducirlo, y si el modelo muestra *underfitting* debemos aumentarlo. Similar al hiperparámetro `C=`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdbfHfW--LdO"
      },
      "source": [
        "# Regresión con SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjnZWyYoEc_x"
      },
      "source": [
        "SVM es potente se puede aplicar a muchos problemas de clasificación, pero, además, es muy versatil. SVM soporta regresion lineal y no lineal.\n",
        "\n",
        "La idea consiste en invertir el objetivo del clasificador. En lugar de intentar conseguir la *calle* más ancha y con el menor número de violaciones, el regresor intenta introducir en la *calle* tantas instancias como sea posible, y, limitar el número de violaciones (instancias fuera del margen).\n",
        "\n",
        "El hiperparámetro $\\epsilon$ controla la anchura de la *calle*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYHInPCJC9m9"
      },
      "source": [
        "## Ejercicio:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caBfzMAkDDEQ"
      },
      "source": [
        "Utilizar la clase *Epsilon Support Vector Regression*, `SVR()` para construir y representar un regresor para los datos generados siguientes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxW_MFxyzbJp"
      },
      "source": [
        "np.random.seed(42)\n",
        "m = 100\n",
        "X = 2 * np.random.rand(m, 1) - 1\n",
        "y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WQjMyoeEGLK"
      },
      "source": [
        "Obtenga resultados para varios valores de $\\epsilon$ comenzando por el valor por defecto.\n",
        "\n",
        "Compruebe también el efecto de parámetro `C=` con valores de 100 y de 0.01."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmF1h1w1Dvb5"
      },
      "source": [
        "*A partir de aquí deben realizar:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B5VCm8c_xfJ"
      },
      "source": [
        "plt.scatter(X, y);\n",
        "#plt.axis([-1, 1, 0, 1])\n",
        "plt.xlabel(r'$x_1$', fontsize='15')\n",
        "plt.ylabel(r'$y$', fontsize='15');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tVWM-wo-WxD"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"auto\")\n",
        "reg.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i96ovOgQ-g6q"
      },
      "source": [
        "def plot_svm_regression(svm_reg, X, y, axes):\n",
        "    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)\n",
        "    y_pred = svm_reg.predict(x1s)\n",
        "    plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
        "    plt.plot(x1s, y_pred + svm_reg.epsilon, \"k--\")\n",
        "    plt.plot(x1s, y_pred - svm_reg.epsilon, \"k--\")\n",
        "    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors='#FFAAAA')\n",
        "    plt.plot(X, y, \"bo\")\n",
        "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
        "    plt.legend(loc=\"upper center\", fontsize=18)\n",
        "    plt.axis(axes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZiJMnXNCUE7"
      },
      "source": [
        "plot_svm_regression(reg, X, y, [-1, 1, 0, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHSDm-LQCbDK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}